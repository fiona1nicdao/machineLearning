chapter 3: a tour of Machine Learning clasifier usign scikit-learn 
 - no free lunches: we don't konw which is the best algorithm for any dataset 
 -- no classifier works best across all scenarios 

scikit learn python code 
- has datasets for you 
- can split the data  to 70% training and 30% testing 
- can import algorithms ex: percepron 
-- fit method is the training / learning 
- can make predictions and use the test set with .predict method 
- you need to try different algorithms for your dataset and see which one works best 

Modeling clas probablity 
- build a model that will give the probablity of the outcome 
- set a threshold for yes will class label or not not the class label 
- odd ratio = probablity of p / probablity of not p 
- log of the odds ratio of p = logit(p) = log(p / 1 -p)

Logit function 
- 

Modeling logit function 
- weight is large = then the feature matters a lot of the outcome 
- 

Logistic Sigmoid - imporant function in machine learning !!!
- will always output a value between 0 and 1  use to learing ? 
why is having a value between 0 and 1 imporant ?? / useful 

-------------------
Sept  26, 2024  Chapter 3
Logistic regression model 
- we will work on log function 
= modeling logit function : model the probablity of an outcome 
ex: model the probablity it will rain 
w0x0 : 
x : features / quantities  
- logistic sigmoid : logit of p 
- convert the range from zero and one 
- no matter what z value is will be between 1 and 0 

Relationship with Adaline 
- net input = scaler / any  real number 
- use sigmoid function change the number between 1 - 0 
- only use sigmoid function to model for probablity 

probablity distribution over classes 
- threshold on the net-input (Z) OR  eta(z ) sigmoid function 
-threshold to get quantizer : we pick the threshold 

Learning the weights 
- some way to show how good our model is 
- cost function for log regression : define the -liklihood L 
 -- look at slide 11
 -- binary cost entropy ? train neural networks ?
 -- loss functio is another name 
 -- "for" large pie = signifies the operation of multiplying a series of number s together 
L(w) is a high number than a good thing b/c training is going well 

Log-likelihood function 
- maximize the likelihood function 

Cost function 
- want to minimize the cost function 
- rewrite likelihood as a cost function / can now to minimize using gradient descent 
- 

Overfitting 
- video : overfitting vs underfitting 
- ex: machine learning for russian tank / us tanks - overfitting with night and day 
- overfitting performs well on training data but doesn't generalized well to unseen data (test data)
    if a model suffers from overfitting, the model has a high varience 
    this is often cuased by a model that's too complex 
- underfitting : can also occur (high bias)
    underfitting ... 
Regularization : adding information in order to solve an ill-posed problem or to prevent overfitting 
    we want the model to generalize NOT fit the training data 
    a good compromise is good ! 
    ex: nueral networks - memorize the training data : we want to learn from the data 
    Regularization is to prevent overfitting 
    noise = errors in the data 

L2 Regularization
- lamba so that the one of the weights do not grow out of proportion 
- so because we are minimize the cost function and we need to minimize the weights from growing out of hand 
- lamba is a constant = how much regulation we apply (we choose the number)
    how to choose a good lamba ? 
    bias = random state ? 
w is the model parameters 
hyper parameters : learning rate , lamba , etc - we set ourselves 

Weight decay 
Regularization parameters 
    C = 1/ lamba

Regulation illustrated 
- 

Support Vector Machine 
- SVM similar to linear regression - different loss function
- b/c of deep learn so SVM are not used as often 
- Maximum margin classification 

Maximum margin classification 
- line / plane / hyperplane
- plane is to 
- what are suprort vectors : pts that bound the discision boundary / margin  to maximize the margin 
- use opimziation of decision line by max the margin 
- support vector : the pts that define the decision boundary 

logistic regression - know well1!!
SVM know the pictures !!! okay / math not soo important 

Mathematrical intuition - how to caputure the hyperplane 
the weights of the define the margin 

Constrained optimization problem 
- capture the SVM Mathematicaly 

SVM Solution -skip 

Exclusive OR (XOR) linear separability 
- no way to separate the blue and red dots 

Turn non-separable classes are separable 
- using the linear classeifier 
= use a line / plan to separate 
- how we got to 3D ?? mapping function 

Generated XOR data 
- how we got to the 3D from 2D

_____________________
Oct 3rd 2024 
Maximum margin classification 
    look at figures and memorize and be able to explain !! 
    want the margin to be Maximumized 
    support vectors define the margins / decision boundary
Mathematical intuition 
    know the basics 
    positive and negative hyperplanes that are parallel to the decisions boundary, whihc can be expressed as follows: 
    minimize || w||
constrained optimization problem 
    minimize = 1/2||w|| ^2
    subject to contrains that the samples are classified correctly
    all the positive examples on the positive side 
    all the negative exmaples on the negative side 
    software libraries to do this 
SVM solutions 
    look at the formula of classifier and weights 
    w of the model depend on the training examples and depends on the labels and have a costant / variable alpha 
    w is the summation of all the training examples of alpha times labels times features 
    alpha is zero for all the examples EXCEPT for the support vectors 
    don't know in advance the support vectors / alpha  - running the numerial optimization you will find it but won't know for the course 
    what are the support vectors ??? EXAM QUESTION 
Slack variables / soft margin SVM
    slack variables : allow some instances to fall off the margin, but panalize them 
    how to many claffication alorgirthms more powerful 
    for non-linear classifiers 
    don't need to know the math 
Extending SVM to non-linearly separable clases
Regularization in SVMS
    logistic regression C : 1.0 
    Linear SVC C : 1.0
REgularization in SVMs
    how wide the margin are 
    C is a hyperparameter
    always use the C for the soft margin / don't normally use C the original way 
Exclusive OR (XOR) linear separability 
    this data is not linearly separatable 
    1950s wrote a book that nerual networks will not work on data that is NOT linearly separatable -- yes for perceptron but NOT true for nueral networkds b/c of XOR function 
Generated XOR data 
    kernal methods create non-linear combination of the original features 
    probablity
    mapping function : hypotenus / patheryon thermo
    d1 =  x1 , d2 x2 , d3 x1^2 + x2^2
    d = demension 
    making the hyperplane 
    use a linear separable to make non-linear data "linear" 
Kernel trick 
    not on the exam
    automate /
Kernel Trick: example 
    dot product of (x * z)^2
    regulation is still important : russian bomber used tires to trick the Machine learning - by overfitting 
K-nearest neighbors 
    save model parameters = weights ******** EXAM 2 answer 
    KNN does not have any weights / non-parametric model 
    once you train the data you don't need the training data anymore 
    model is a approcimation / zipfile - loss-less compression 
    KNN there is not training / the training data is the model 
    SO CANNOT throwout the training data 
    good for webbase more that takes in new data all the time 
    NO training 
    BUT there is TRAINING data ! 
Basic KNN algorithm
    choose ... 
slide 42 
    3 classifiers : predict and look at the neighbors about it 
    more examples around it is from a particular class so predict it is from this 
    look at "k" examples around the predicted and count and assign the class of the predicted to the "?" is from the most counted classifer 
    in practice don't have super "close" example to the predicted one "?"
    define the circle = KNN algorithm use it to define it / you need defien spacial values / you need a distance measurements / some way to learn simularity 
KNN advantages 
    need distance metrics 
    eucidean distance 
    manhattan distance 
    cos/sin - angle of the predict 

the model is the final weights / 

HW3 : working on the classifiers 
C represent regulation of the weights / controls the strength of the model / russian tank example 
try C = 0, 1, 10, 0.0001, 0.001 --  1, 10, 100, 10000 
need to save the model for regression ! from sklearn 

EXAM
    ask about SVM
    ask about : rising to a higer demnsional / turn non-separable classes are separable(34)
    first transform to 3 d 
    slide (35) 
    logistic regression are simlar to SVM
    implementing no libraries / 